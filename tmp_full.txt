from __future__ import annotations

import json
import logging
import os as _os
import random
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Iterable, List, Optional, Tuple

import numpy as np

# Standardize math defaults to float32 end-to-end
import torch  # type: ignore


logger = logging.getLogger(__name__)


def set_all_seeds(seed: int = 2024) -> None:
    """Compatibility wrapper around the core RNG seeding helper."""

    _core_set_all_seeds(int(seed))


class PmarloApiIncompatibilityError(RuntimeError):
    """Raised when mlcolvar API layout does not expose expected classes."""



def _resolve_activation_module(name: str):
    return core_resolve_activation_module(name)


def _normalize_hidden_dropout(spec: Any, num_hidden: int) -> List[float]:
    return core_normalize_hidden_dropout(spec, num_hidden)


def _override_core_mlp(
    core,
    layers,
    activation_name: str,
    linear_head: bool,
    *,
    hidden_dropout: Any = None,
    layer_norm_hidden: bool = False,
) -> None:
    core_override_core_mlp(
        core,
        layers,
        activation_name,
        linear_head,
        hidden_dropout=hidden_dropout,
        layer_norm_hidden=layer_norm_hidden,
    )


def _apply_output_whitening(
    net,
    Z,
    idx_tau,
    *,
    apply: bool = False,
    eig_floor: float = 1e-4,
):
    return core_apply_output_whitening(net, Z, idx_tau, apply=apply, eig_floor=eig_floor)


def _construct_deeptica_core(cfg: Any, scaler: StandardScaler):
    core, _ = core_construct_deeptica_core(cfg, scaler)
    return core


def _resolve_hidden_layers(cfg: Any) -> tuple[int, ...]:
    return core_resolve_hidden_layers(cfg)


def _wrap_with_preprocessing_layers(core: Any, cfg: Any, scaler: StandardScaler):
    return core_wrap_with_preprocessing_layers(core, cfg, scaler)


def _resolve_input_dropout(cfg: Any) -> float:
    return core_resolve_input_dropout(cfg)


def _strip_batch_norm(module: Any) -> None:
    core_strip_batch_norm(module)


class _PrePostWrapper(_CorePrePostWrapper):
    pass


class _WhitenWrapper(_CoreWhitenWrapper):
    pass
# Official DeepTICA import and helpers (mlcolvar>=1.2)
try:  # pragma: no cover - optional extra
    from mlcolvar.cvs import DeepTICA  # type: ignore
    from mlcolvar.utils.timelagged import (
        create_timelagged_dataset as _create_timelagged_dataset,  # type: ignore
    )
except Exception as e:  # pragma: no cover - optional extra
    if isinstance(e, ImportError):
        raise ImportError("Install optional extra pmarlo[mlcv] to use Deep-TICA") from e
    raise PmarloApiIncompatibilityError(
        "mlcolvar installed but DeepTICA not found in expected locations"
    ) from e

# External scaling via scikit-learn (avoid internal normalization)
from sklearn.preprocessing import StandardScaler  # type: ignore

from pmarlo.ml.deeptica.whitening import apply_output_transform
from .core import (
from .core.model import (
    WhitenWrapper as _CoreWhitenWrapper,
    apply_output_whitening as core_apply_output_whitening,
    construct_deeptica_core as core_construct_deeptica_core,
    normalize_hidden_dropout as core_normalize_hidden_dropout,
    override_core_mlp as core_override_core_mlp,
    resolve_activation_module as core_resolve_activation_module,
    resolve_hidden_layers as core_resolve_hidden_layers,
    resolve_input_dropout as core_resolve_input_dropout,
    strip_batch_norm as core_strip_batch_norm,
    wrap_with_preprocessing_layers as core_wrap_with_preprocessing_layers,
    PrePostWrapper as _CorePrePostWrapper,
)

from .core.model import (
    WhitenWrapper as _CoreWhitenWrapper,
    apply_output_whitening as core_apply_output_whitening,
    construct_deeptica_core as core_construct_deeptica_core,
    normalize_hidden_dropout as core_normalize_hidden_dropout,
    override_core_mlp as core_override_core_mlp,
    resolve_activation_module as core_resolve_activation_module,
    resolve_hidden_layers as core_resolve_hidden_layers,
    resolve_input_dropout as core_resolve_input_dropout,
    strip_batch_norm as core_strip_batch_norm,
    wrap_with_preprocessing_layers as core_wrap_with_preprocessing_layers,
    PrePostWrapper as _CorePrePostWrapper,
)


