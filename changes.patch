diff --git a/example_programs/app_usecase/app/app.py b/example_programs/app_usecase/app/app.py
index 8788a82948ce3f2771ce7d75c091f4c5f171a608..612ae71e264dc022cb790d3dde4250a11b9db009 100644
--- a/example_programs/app_usecase/app/app.py
+++ b/example_programs/app_usecase/app/app.py
@@ -328,77 +328,93 @@ def main() -> None:
                     st.info("No recorded runs available yet.")
 
         if sim is not None:
             st.success(
                 f"Latest run {sim.run_id} produced {len(sim.traj_files)} "
                 f"trajectories across {len(sim.analysis_temperatures)} temperatures."
             )
             st.caption(f"Workspace: {sim.run_dir}")
             with st.expander("Run outputs", expanded=False):
                 st.json(
                     {
                         "run_id": sim.run_id,
                         "trajectories": [p.name for p in sim.traj_files],
                         "analysis_temperatures": sim.analysis_temperatures,
                     }
                 )
             st.subheader("Emit shards from the latest run")
             with st.form("emit_shards_form"):
                 stride = st.number_input(
                     "Stride (frames)",
                     min_value=1,
                     value=5,
                     step=1,
                     key="emit_stride",
                 )
+                frames_per_shard = st.number_input(
+                    "Frames per shard",
+                    min_value=500,
+                    value=5000,
+                    step=500,
+                    key="frames_per_shard",
+                )
+                hop_frames = st.number_input(
+                    "Hop (overlap step)",
+                    min_value=0,
+                    value=5000,
+                    step=500,
+                    key="hop_frames",
+                )
                 temp_default = sim.analysis_temperatures[0] if sim.analysis_temperatures else 300.0
                 shard_temp = st.number_input(
                     "Shard metadata temperature (K)",
                     min_value=0.0,
                     value=float(temp_default),
                     step=5.0,
                     key="emit_temperature",
                 )
                 seed_start = st.number_input(
                     "Shard ID seed start",
                     min_value=0,
                     value=0,
                     step=1,
                     key="emit_seed_start",
                 )
                 reference_path = st.text_input(
                     "Reference PDB for RMSD (optional)",
                     value="",
                     key="emit_reference",
                 )
                 emit = st.form_submit_button("Emit shard files")
                 if emit:
                     try:
                         request = ShardRequest(
                             stride=int(stride),
                             temperature=float(shard_temp),
                             seed_start=int(seed_start),
+                            frames_per_shard=int(frames_per_shard),
+                            hop_frames=int(hop_frames) if hop_frames > 0 else None,
                             reference=Path(reference_path).expanduser().resolve()
                             if reference_path.strip()
                             else None,
                         )
                         shard_result = backend.emit_shards(
                             sim,
                             request,
                             provenance={"source": "app_usecase"},
                         )
                         st.session_state[_LAST_SHARDS] = shard_result
                         st.success(
                             f"Emitted {shard_result.n_shards} shards "
                             f"({shard_result.n_frames} frames)."
                         )
                         st.json(
                             {
                                 "directory": str(shard_result.shard_dir),
                                 "files": [p.name for p in shard_result.shard_paths],
                             }
                         )
                     except Exception as exc:
                         st.error(f"Shard emission failed: {exc}")
 
     with tab_training:
         st.header("Train collective-variable model")
diff --git a/example_programs/app_usecase/app/backend.py b/example_programs/app_usecase/app/backend.py
index af0b58cc48c00742641db37b7c5363a943902afd..a7675f4984561502bafb660f620feb4e4766c6bc 100644
--- a/example_programs/app_usecase/app/backend.py
+++ b/example_programs/app_usecase/app/backend.py
@@ -3,51 +3,55 @@ from __future__ import annotations
 """Backend utilities powering the Streamlit joint-learning demo.
 
 The previous iteration of the app mixed UI callbacks, shard bookkeeping, and
 engine calls in a single ~900 line module. This rewrite keeps the backend
 focused on three responsibilities:
 
 1. manage the on-disk workspace layout (sims -> shards -> models -> bundles)
 2. provide thin orchestration wrappers around the high-level
    :mod:`pmarlo.api` helpers that already implement REMD, shard emission, and
    MSM/FES builds
 3. persist lightweight manifest entries so the UI can remain mostly stateless
 
 The goal is to make it straightforward to express the interactive workflow::
 
     sample -> emit shards -> train CV model -> enrich dataset -> build MSM/FES
 
 while keeping the logic reusable for non-UI automation in the future.
 """
 
 from dataclasses import dataclass, field
 from datetime import datetime
 from pathlib import Path
 import shutil
 from typing import Any, Dict, Iterable, List, Optional, Sequence
 
-from pmarlo.api import build_from_shards, emit_shards_rg_rmsd, run_replica_exchange
+from pmarlo.api import (
+    build_from_shards,
+    emit_shards_rg_rmsd_windowed,
+    run_replica_exchange,
+)
 from pmarlo.data.shard import read_shard
 from pmarlo.transform.build import BuildResult, _sanitize_artifacts
 
 try:  # Package-relative when imported as module
     from .state import StateManager
 except ImportError:  # Fallback for direct script import
     import sys
 
     _APP_DIR = Path(__file__).resolve().parent
     if str(_APP_DIR) not in sys.path:
         sys.path.insert(0, str(_APP_DIR))
     from state import StateManager  # type: ignore
 
 __all__ = [
     "WorkspaceLayout",
     "SimulationConfig",
     "SimulationResult",
     "ShardRequest",
     "ShardResult",
     "TrainingConfig",
     "TrainingResult",
     "BuildConfig",
     "BuildArtifact",
     "WorkflowBackend",
     "choose_sim_seed",
@@ -172,61 +176,65 @@ class SimulationConfig:
     random_seed: Optional[int] = None
     label: Optional[str] = None
     jitter_start: bool = False
     jitter_sigma_A: float = 0.05
     exchange_frequency_steps: Optional[int] = None
     temperature_schedule_mode: Optional[str] = None
 
 
 @dataclass
 class SimulationResult:
     run_id: str
     run_dir: Path
     pdb_path: Path
     traj_files: List[Path]
     analysis_temperatures: List[float]
     steps: int
     created_at: str
 
 
 @dataclass
 class ShardRequest:
     stride: int = 5
     temperature: float = 300.0
     reference: Optional[Path] = None
     seed_start: int = 0
+    frames_per_shard: int = 5000
+    hop_frames: Optional[int] = None
 
 
 @dataclass
 class ShardResult:
     run_id: str
     shard_dir: Path
     shard_paths: List[Path]
     n_frames: int
     n_shards: int
     temperature: float
     stride: int
+    frames_per_shard: int
+    hop_frames: Optional[int]
     created_at: str
 
 
 @dataclass
 class TrainingConfig:
     lag: int = 5
     bins: Dict[str, int] = field(default_factory=lambda: {"Rg": 64, "RMSD_ref": 64})
     seed: int = 1337
     temperature: float = 300.0
     hidden: Sequence[int] = (128, 128)
     max_epochs: int = 200
     early_stopping: int = 25
 
     def deeptica_params(self) -> Dict[str, Any]:
         return {
             "lag": int(max(1, self.lag)),
             "n_out": 2,
             "hidden": tuple(int(h) for h in self.hidden),
             "max_epochs": int(self.max_epochs),
             "early_stopping": int(self.early_stopping),
             "reweight_mode": "scaled_time",
         }
 
 
 @dataclass
@@ -307,88 +315,106 @@ class WorkflowBackend:
                 "quick": bool(config.quick),
                 "random_seed": int(config.random_seed)
                 if config.random_seed is not None
                 else None,
                 "traj_files": [str(p) for p in result.traj_files],
                 "created_at": created,
             }
         )
         return result
 
     def emit_shards(
         self,
         simulation: SimulationResult,
         request: ShardRequest,
         *,
         provenance: Optional[Dict[str, Any]] = None,
     ) -> ShardResult:
         shard_dir = self.layout.shards_dir / simulation.run_id
         shard_dir.mkdir(parents=True, exist_ok=True)
         note = {
             "run_id": simulation.run_id,
             "analysis_temperatures": simulation.analysis_temperatures,
         }
         if provenance:
             note.update(provenance)
-        shard_paths = emit_shards_rg_rmsd(
+        shard_paths = emit_shards_rg_rmsd_windowed(
             pdb_file=simulation.pdb_path,
             traj_files=[str(p) for p in simulation.traj_files],
             out_dir=str(shard_dir),
             reference=str(request.reference) if request.reference else None,
             stride=int(max(1, request.stride)),
             temperature=float(request.temperature),
             seed_start=int(max(0, request.seed_start)),
+            frames_per_shard=int(max(1, request.frames_per_shard)),
+            hop_frames=(
+                int(request.hop_frames)
+                if request.hop_frames is not None and request.hop_frames > 0
+                else None
+            ),
             provenance=note,
         )
         shard_paths = _coerce_path_list(shard_paths)
         n_frames = 0
         for path in shard_paths:
             try:
                 meta, _, _ = read_shard(path)
                 n_frames += int(getattr(meta, "n_frames", 0))
             except Exception:
                 continue
         created = _timestamp()
         result = ShardResult(
             run_id=simulation.run_id,
             shard_dir=shard_dir.resolve(),
             shard_paths=shard_paths,
             n_frames=int(n_frames),
             n_shards=len(shard_paths),
             temperature=float(request.temperature),
             stride=int(max(1, request.stride)),
+            frames_per_shard=int(max(1, request.frames_per_shard)),
+            hop_frames=(
+                int(request.hop_frames)
+                if request.hop_frames is not None and request.hop_frames > 0
+                else None
+            ),
             created_at=created,
         )
         self.state.append_shards(
             {
                 "run_id": simulation.run_id,
                 "directory": str(result.shard_dir),
                 "paths": [str(p) for p in shard_paths],
                 "temperature": float(request.temperature),
                 "stride": int(max(1, request.stride)),
                 "n_shards": len(shard_paths),
                 "n_frames": int(n_frames),
+                "frames_per_shard": int(max(1, request.frames_per_shard)),
+                "hop_frames": (
+                    int(request.hop_frames)
+                    if request.hop_frames is not None and request.hop_frames > 0
+                    else None
+                ),
                 "created_at": created,
             }
         )
         return result
 
     # ------------------------------------------------------------------
     # Discovery helpers
     # ------------------------------------------------------------------
     def discover_shards(self) -> List[Path]:
         if not self.layout.shards_dir.exists():
             return []
         return sorted(self.layout.shards_dir.rglob("*.json"))
 
     def shard_summaries(self) -> List[Dict[str, Any]]:
         info: List[Dict[str, Any]] = []
         for entry in self.state.shards:
             info.append(dict(entry))
         return info
 
     # ------------------------------------------------------------------
     # Model training and analysis
     # ------------------------------------------------------------------
     def train_model(
         self,
         shard_jsons: Sequence[Path],
diff --git a/src/pmarlo/api.py b/src/pmarlo/api.py
index b0b96e53d2996d0d1d7cb59ea435eec1e0f814ca..152b55a96c712e799801c1ffe52a543fbc708a81 100644
--- a/src/pmarlo/api.py
+++ b/src/pmarlo/api.py
@@ -1342,50 +1342,216 @@ def find_conformations_with_msm(
     requested_pair: Optional[Tuple[str, str]] = None,
     traj_stride: int = 1,
     atom_selection: str | Sequence[int] | None = None,
     chunk_size: int = 1000,
 ) -> Path:
     """One-line convenience wrapper to find representative conformations.
 
     This is a thin alias around :func:`find_conformations` to mirror the
     example program name and make the public API more discoverable.
     """
     return find_conformations(
         topology_pdb=topology_pdb,
         trajectory_choice=trajectory_file,
         output_dir=output_dir,
         feature_specs=feature_specs,
         requested_pair=requested_pair,
         traj_stride=traj_stride,
         atom_selection=atom_selection,
         chunk_size=chunk_size,
     )
 
 
 # ------------------------------ App-friendly wrappers ------------------------------
 
 
+def emit_shards_rg_rmsd_windowed(
+    pdb_file: str | Path,
+    traj_files: list[str | Path],
+    out_dir: str | Path,
+    *,
+    reference: str | Path | None = None,
+    stride: int = 1,
+    temperature: float = 300.0,
+    seed_start: int = 0,
+    frames_per_shard: int = 5000,
+    hop_frames: int | None = None,
+    progress_callback=None,
+    provenance: dict | None = None,
+) -> list[Path]:
+    """Emit many overlapping shards per trajectory via a sliding window."""
+
+    import mdtraj as md  # type: ignore
+    import numpy as np
+
+    from pmarlo.data.shard import write_shard  # type: ignore
+    from pmarlo.io import trajectory as _traj_io  # type: ignore
+    from pmarlo.transform.progress import ProgressReporter  # type: ignore
+
+    pdb_file = Path(pdb_file)
+    out_dir = Path(out_dir)
+    out_dir.mkdir(parents=True, exist_ok=True)
+
+    top0 = md.load(str(pdb_file))
+    ref = (
+        md.load(str(reference), top=str(pdb_file))[0]
+        if reference is not None and Path(reference).exists()
+        else top0[0]
+    )
+    ca_sel = top0.topology.select("name CA")
+    ca_sel = ca_sel if ca_sel.size else None
+
+    existing: list[int] = []
+    for shard_file in sorted(out_dir.glob("shard_*.json")):
+        try:
+            existing.append(int(shard_file.stem.split("_")[1]))
+        except Exception:
+            continue
+    next_idx = (max(existing) + 1) if existing else 0
+    start_idx = next_idx
+    seed_base = int(seed_start) + start_idx
+
+    reporter = ProgressReporter(progress_callback)
+
+    def _emit(event: str, data: dict) -> None:
+        try:
+            reporter.emit(event, data)
+        except Exception:
+            pass
+
+    shard_paths: list[Path] = []
+    files = [Path(p) for p in traj_files]
+    files.sort()
+    _emit(
+        "emit_begin",
+        {
+            "n_inputs": len(files),
+            "out_dir": str(out_dir),
+            "temperature": float(temperature),
+            "current": 0,
+            "total": len(files),
+        },
+    )
+
+    window = max(1, int(frames_per_shard))
+    hop = max(1, int(hop_frames) if hop_frames is not None else window)
+
+    for index, traj_path in enumerate(files):
+        _emit(
+            "emit_one_begin",
+            {
+                "index": int(index),
+                "traj": str(traj_path),
+                "current": int(index + 1),
+                "total": int(len(files)),
+            },
+        )
+
+        rg_parts: list[np.ndarray] = []
+        rmsd_parts: list[np.ndarray] = []
+        total_frames = 0
+        for chunk in _traj_io.iterload(
+            str(traj_path),
+            top=str(pdb_file),
+            stride=int(max(1, stride)),
+            chunk=1000,
+        ):
+            try:
+                chunk = chunk.superpose(ref, atom_indices=ca_sel)
+            except Exception:
+                pass
+            rg_parts.append(md.compute_rg(chunk).astype(np.float64))
+            rmsd_parts.append(
+                md.rmsd(chunk, ref, atom_indices=ca_sel).astype(np.float64)
+            )
+            total_frames += int(chunk.n_frames)
+
+        if rg_parts:
+            rg = np.concatenate(rg_parts)
+        else:
+            rg = np.zeros((0,), dtype=np.float64)
+        if rmsd_parts:
+            rmsd = np.concatenate(rmsd_parts)
+        else:
+            rmsd = np.zeros((0,), dtype=np.float64)
+
+        n_frames = int(rg.shape[0])
+        if n_frames >= window:
+            for start in range(0, n_frames - window + 1, hop):
+                stop = start + window
+                shard_id = f"shard_{next_idx:04d}"
+                cvs = {
+                    "Rg": rg[start:stop],
+                    "RMSD_ref": rmsd[start:stop],
+                }
+                source: dict[str, object] = {
+                    "traj": str(traj_path),
+                    "range": [int(start), int(stop)],
+                    "n_frames": int(stop - start),
+                }
+                if provenance:
+                    try:
+                        merged = dict(provenance)
+                        merged.update(source)
+                        source = merged
+                    except Exception:
+                        pass
+                shard_path = write_shard(
+                    out_dir=out_dir,
+                    shard_id=shard_id,
+                    cvs=cvs,
+                    dtraj=None,
+                    periodic={"Rg": False, "RMSD_ref": False},
+                    seed=int(seed_base + (next_idx - start_idx)),
+                    temperature=float(temperature),
+                    source=source,
+                )
+                shard_paths.append(shard_path.resolve())
+                next_idx += 1
+
+        _emit(
+            "emit_one_end",
+            {
+                "index": int(index),
+                "traj": str(traj_path),
+                "frames": int(total_frames),
+                "current": int(index + 1),
+                "total": int(len(files)),
+            },
+        )
+
+    _emit(
+        "emit_end",
+        {
+            "n_shards": len(shard_paths),
+            "current": int(len(files)),
+            "total": int(len(files)),
+        },
+    )
+    return shard_paths
+
+
 def emit_shards_rg_rmsd(
     pdb_file: str | Path,
     traj_files: list[str | Path],
     out_dir: str | Path,
     *,
     reference: str | Path | None = None,
     stride: int = 1,
     temperature: float = 300.0,
     seed_start: int = 0,
     progress_callback=None,
     provenance: dict | None = None,
 ) -> list[Path]:
     """Stream trajectories and emit shards with Rg and RMSD to a reference.
 
     This is a convenience wrapper for UI apps. It handles quiet streaming via
     pmarlo.io.trajectory.iterload, alignment to a global reference, and writes
     deterministic shards under ``out_dir``.
     """
     import mdtraj as md  # type: ignore
 
     # Quiet streaming iterator for reading DCDs without plugin chatter
     from pmarlo.io import trajectory as _traj_io
 
     pdb_file = Path(pdb_file)
     out_dir = Path(out_dir)
diff --git a/src/pmarlo/features/deeptica/__init__.py b/src/pmarlo/features/deeptica/__init__.py
index b35906ced93902974ceae17ad2635b6cc07ece9d..9b14bb6a72e0f123b94e594b8136489104ba86f7 100644
--- a/src/pmarlo/features/deeptica/__init__.py
+++ b/src/pmarlo/features/deeptica/__init__.py
@@ -1,33 +1,33 @@
 from __future__ import annotations
 
 import json
 import os as _os
 import random
 from dataclasses import asdict, dataclass
 from pathlib import Path
-from typing import Any, List, Optional, Tuple
+from typing import Any, Iterable, List, Optional, Tuple
 
 import numpy as np
 
 # Standardize math defaults to float32 end-to-end
 import torch  # type: ignore
 
 torch.set_float32_matmul_precision("high")
 torch.set_default_dtype(torch.float32)
 
 
 def set_all_seeds(seed: int = 2024) -> None:
     """Set RNG seeds across Python, NumPy, and Torch (CPU/GPU)."""
     random.seed(int(seed))
     np.random.seed(int(seed))
     torch.manual_seed(int(seed))
     if (
         hasattr(torch, "cuda") and torch.cuda.is_available()
     ):  # pragma: no cover - optional
         try:
             torch.cuda.manual_seed_all(int(seed))
         except Exception:
             pass
 
 
 class PmarloApiIncompatibilityError(RuntimeError):
@@ -50,53 +50,130 @@ except Exception as e:  # pragma: no cover - optional extra
     ) from e
 
 # External scaling via scikit-learn (avoid internal normalization)
 from sklearn.preprocessing import StandardScaler  # type: ignore
 
 from .losses import VAMP2Loss
 
 
 def _resolve_activation_module(name: str):
     import torch.nn as _nn  # type: ignore
 
     key = (name or "").strip().lower()
     if key in {"gelu", "gaussian"}:
         return _nn.GELU()
     if key in {"relu", "relu+"}:
         return _nn.ReLU()
     if key in {"elu"}:
         return _nn.ELU()
     if key in {"selu"}:
         return _nn.SELU()
     if key in {"leaky_relu", "lrelu"}:
         return _nn.LeakyReLU()
     return _nn.Tanh()
 
 
-def _override_core_mlp(core, layers, activation_name: str, linear_head: bool) -> None:
-    """Override core MLP configuration."""
-    pass
+def _normalize_hidden_dropout(spec: Any, num_hidden: int) -> List[float]:
+    """Expand a dropout specification to match the number of hidden transitions."""
+
+    if num_hidden <= 0:
+        return []
+
+    values: List[float]
+    if spec is None:
+        values = [0.0] * num_hidden
+    elif isinstance(spec, (int, float)) and not isinstance(spec, bool):
+        values = [float(spec)] * num_hidden
+    elif isinstance(spec, str):
+        try:
+            scalar = float(spec)
+        except ValueError:
+            scalar = 0.0
+        values = [scalar] * num_hidden
+    else:
+        values = []
+        if isinstance(spec, Iterable) and not isinstance(spec, (bytes, str)):
+            for item in spec:
+                try:
+                    values.append(float(item))
+                except Exception:
+                    values.append(0.0)
+        else:
+            try:
+                scalar = float(spec)
+            except Exception:
+                scalar = 0.0
+            values = [scalar] * num_hidden
+
+    if not values:
+        values = [0.0] * num_hidden
+
+    if len(values) < num_hidden:
+        last = values[-1]
+        values = values + [last] * (num_hidden - len(values))
+    elif len(values) > num_hidden:
+        values = values[:num_hidden]
+
+    return [float(max(0.0, min(1.0, v))) for v in values]
+
+
+def _override_core_mlp(
+    core,
+    layers,
+    activation_name: str,
+    linear_head: bool,
+    *,
+    hidden_dropout: Any = None,
+    layer_norm_hidden: bool = False,
+) -> None:
+    """Override core MLP configuration with custom activations/dropout."""
+
+    if linear_head or len(layers) <= 2:
+        return
+    try:
+        import torch.nn as _nn  # type: ignore
+    except Exception:
+        return
+
+    hidden_transitions = max(0, len(layers) - 2)
+    dropout_values = _normalize_hidden_dropout(hidden_dropout, hidden_transitions)
+
+    modules: list[_nn.Module] = []
+    for idx in range(len(layers) - 1):
+        in_features = int(layers[idx])
+        out_features = int(layers[idx + 1])
+        modules.append(_nn.Linear(in_features, out_features))
+        if idx < len(layers) - 2:
+            if layer_norm_hidden:
+                modules.append(_nn.LayerNorm(out_features))
+            modules.append(_resolve_activation_module(activation_name))
+            drop_p = dropout_values[idx] if idx < len(dropout_values) else 0.0
+            if drop_p > 0.0:
+                modules.append(_nn.Dropout(p=float(drop_p)))
+
+    if modules:
+        core.nn = _nn.Sequential(*modules)  # type: ignore[attr-defined]
 
 
 def _apply_output_whitening(
     net, Z, idx_tau, *, apply: bool = False, eig_floor: float = 1e-4
 ):
     import torch
 
     tensor = torch.as_tensor(Z, dtype=torch.float32)
     with torch.no_grad():
         outputs = net(tensor)
         if isinstance(outputs, torch.Tensor):
             outputs = outputs.detach().cpu().numpy()
     if outputs is None or outputs.size == 0:
         info = {
             "output_variance": [],
             "var_zt": [],
             "cond_c00": None,
             "cond_ctt": None,
             "mean": [],
             "transform": [],
             "transform_applied": bool(apply),
         }
         return net, info
 
     mean = np.mean(outputs, axis=0)
@@ -135,122 +212,116 @@ def _apply_output_whitening(
         var_zt = tau_center.var(axis=0, ddof=1).astype(float).tolist()
         n_tau = max(1, tau_center.shape[0] - 1)
         Ct = (tau_center.T @ tau_center) / float(n_tau)
         Ct_reg = _regularize(Ct)
         eig_ct = np.linalg.eigvalsh(Ct_reg)
         eig_ct = np.clip(eig_ct, max(eig_floor, 1e-8), None)
         cond_ctt = float(eig_ct.max() / eig_ct.min())
 
     if var_zt is None:
         var_zt = output_var
 
     transform = inv_sqrt if apply else np.eye(inv_sqrt.shape[0], dtype=np.float64)
     wrapped = _WhitenWrapper(net, mean, transform) if apply else net
 
     info = {
         "output_variance": output_var,
         "var_zt": var_zt,
         "cond_c00": cond_c00,
         "cond_ctt": cond_ctt,
         "mean": mean.astype(float).tolist(),
         "transform": inv_sqrt.astype(float).tolist(),
         "transform_applied": bool(apply),
     }
     return wrapped, info
 
-    if linear_head or len(layers) <= 2:
-        return
-    try:
-        import torch.nn as _nn  # type: ignore
-    except Exception:
-        return
-    modules: list[_nn.Module] = []
-    for idx in range(len(layers) - 1):
-        in_features = int(layers[idx])
-        out_features = int(layers[idx + 1])
-        modules.append(_nn.Linear(in_features, out_features))
-        if idx < len(layers) - 2:
-            modules.append(_resolve_activation_module(activation_name))
-    if modules:
-        core.nn = _nn.Sequential(*modules)  # type: ignore[attr-defined]
-
 
 # Provide a module-level whitening wrapper so helper functions can reference it
 try:
     import torch.nn as _nn  # type: ignore
 except Exception:  # pragma: no cover - optional in environments without torch
     _nn = None  # type: ignore
 
 if _nn is not None:
 
     class _WhitenWrapper(_nn.Module):  # type: ignore[misc]
         def __init__(
             self,
             inner,
             mean: np.ndarray | torch.Tensor,
             transform: np.ndarray | torch.Tensor,
         ):
             super().__init__()
             self.inner = inner
             # Register buffers to move with the module's device
             self.register_buffer("mean", torch.as_tensor(mean, dtype=torch.float32))
             self.register_buffer(
                 "transform", torch.as_tensor(transform, dtype=torch.float32)
             )
 
         def forward(self, x):  # type: ignore[override]
             y = self.inner(x)
             y = y - self.mean
             return torch.matmul(y, self.transform.T)
 
 
 @dataclass(frozen=True)
 class DeepTICAConfig:
     lag: int
     n_out: int = 2
     hidden: Tuple[int, ...] = (32, 16)
     activation: str = "gelu"
-    learning_rate: float = 1e-3
+    learning_rate: float = 3e-4
     batch_size: int = 1024
     max_epochs: int = 200
     early_stopping: int = 25
     weight_decay: float = 1e-4
     log_every: int = 1
     seed: int = 0
     reweight_mode: str = "scaled_time"  # or "none"
     # New knobs for loaders and validation split
     val_frac: float = 0.1
     num_workers: int = 2
     # Optimization and regularization knobs
     lr_schedule: str = "cosine"  # "none" | "cosine"
     warmup_epochs: int = 5
-    dropout: float = 0.1
-    layer_norm_in: bool = True
+    dropout: float = 0.0
+    dropout_input: Optional[float] = None
+    hidden_dropout: Tuple[float, ...] = ()
+    layer_norm_in: bool = False
+    layer_norm_hidden: bool = False
     linear_head: bool = False
     # Dataset splitting/loader control
     val_split: str = "by_shard"  # "by_shard" | "random"
     batches_per_epoch: int = 200
+    gradient_clip_val: float = 1.0
+    gradient_clip_algorithm: str = "norm"
+    tau_schedule: Tuple[int, ...] = ()
+    vamp_eps: float = 1e-3
+    vamp_eps_abs: float = 1e-6
+    vamp_alpha: float = 0.15
+    vamp_cond_reg: float = 1e-4
 
 
 class DeepTICAModel:
     """Thin wrapper exposing a stable API around mlcolvar DeepTICA."""
 
     def __init__(
         self,
         cfg: DeepTICAConfig,
         scaler: Any,
         net: Any,
         *,
         device: str = "cpu",
         training_history: dict | None = None,
     ):
         self.cfg = cfg
         self.scaler = scaler
         self.net = net  # mlcolvar.cvs.DeepTICA
         self.device = str(device)
         self.training_history = dict(training_history or {})
 
     def transform(self, X: np.ndarray) -> np.ndarray:
         Z = self.scaler.transform(np.asarray(X, dtype=np.float64))
         with torch.no_grad():
             try:
                 y = self.net(Z)  # type: ignore[misc]
@@ -308,99 +379,114 @@ class DeepTICAModel:
         path = Path(path)
         cfg = DeepTICAConfig(
             **json.loads(path.with_suffix(".json").read_text(encoding="utf-8"))
         )
         scaler_ckpt = torch.load(path.with_suffix(".scaler.pt"), map_location="cpu")
         scaler = StandardScaler(with_mean=True, with_std=True)
         # Rehydrate the necessary attributes for transform()
         scaler.mean_ = np.asarray(scaler_ckpt["mean"], dtype=np.float64)
         scaler.scale_ = np.asarray(scaler_ckpt["std"], dtype=np.float64)
         # Some sklearn versions also check these, so set conservatively if missing
         try:  # pragma: no cover - attribute presence varies across versions
             scaler.n_features_in_ = int(scaler.mean_.shape[0])  # type: ignore[attr-defined]
         except Exception:
             pass
         # Rebuild network using the official constructor, then wrap with pre/post layers
         in_dim = int(scaler.mean_.shape[0])
         hidden_cfg = tuple(int(h) for h in getattr(cfg, "hidden", ()) or ())
         if bool(getattr(cfg, "linear_head", False)):
             hidden_layers: tuple[int, ...] = ()
         else:
             hidden_layers = hidden_cfg if hidden_cfg else (32, 16)
         layers = [in_dim, *hidden_layers, int(cfg.n_out)]
         activation_name = (
             str(getattr(cfg, "activation", "gelu")).lower().strip() or "gelu"
         )
+        hidden_dropout_cfg: Any = getattr(cfg, "hidden_dropout", ())
+        layer_norm_hidden = bool(getattr(cfg, "layer_norm_hidden", False))
         try:
             core = DeepTICA(
                 layers=layers,
                 n_cvs=int(cfg.n_out),
                 activation=activation_name,
                 options={"norm_in": False},
             )
         except TypeError:
             core = DeepTICA(
                 layers=layers,
                 n_cvs=int(cfg.n_out),
                 options={"norm_in": False},
             )
             _override_core_mlp(
-                core, layers, activation_name, bool(getattr(cfg, "linear_head", False))
+                core,
+                layers,
+                activation_name,
+                bool(getattr(cfg, "linear_head", False)),
+                hidden_dropout=hidden_dropout_cfg,
+                layer_norm_hidden=layer_norm_hidden,
             )
         else:
             _override_core_mlp(
-                core, layers, activation_name, bool(getattr(cfg, "linear_head", False))
+                core,
+                layers,
+                activation_name,
+                bool(getattr(cfg, "linear_head", False)),
+                hidden_dropout=hidden_dropout_cfg,
+                layer_norm_hidden=layer_norm_hidden,
             )
         import torch.nn as _nn  # type: ignore
 
         def _strip_batch_norm(module: _nn.Module) -> None:
             for name, child in module.named_children():
                 if isinstance(child, _nn.modules.batchnorm._BatchNorm):
                     setattr(module, name, _nn.Identity())
                 else:
                     _strip_batch_norm(child)
 
         class _PrePostWrapper(_nn.Module):  # type: ignore[misc]
             def __init__(self, inner, in_features: int, *, ln_in: bool, p_drop: float):
                 super().__init__()
                 self.ln = _nn.LayerNorm(in_features) if ln_in else _nn.Identity()
-                p = float(max(0.0, min(0.1, p_drop)))
+                p = float(max(0.0, min(1.0, p_drop)))
                 self.drop_in = _nn.Dropout(p=p) if p > 0 else _nn.Identity()
                 self.inner = inner
                 self.drop_out = _nn.Identity()
 
             def forward(self, x):  # type: ignore[override]
                 x = self.ln(x)
                 x = self.drop_in(x)
                 return self.inner(x)
 
         _strip_batch_norm(core)
+        dropout_in = getattr(cfg, "dropout_input", None)
+        if dropout_in is None:
+            dropout_in = getattr(cfg, "dropout", 0.1)
         net = _PrePostWrapper(
             core,
             in_dim,
             ln_in=bool(getattr(cfg, "layer_norm_in", True)),
-            p_drop=float(getattr(cfg, "dropout", 0.1)),
+            p_drop=float(dropout_in),
         )
         state = torch.load(path.with_suffix(".pt"), map_location="cpu")
         net.load_state_dict(state["state_dict"])  # type: ignore[index]
         net.eval()
         return cls(cfg, scaler, net)
 
     def to_torchscript(self, path: Path) -> Path:
         path = Path(path)
         path.parent.mkdir(parents=True, exist_ok=True)
         self.net.eval()
         # Trace with single precision (typical for inference)
         example = torch.zeros(1, int(self.scaler.mean_.shape[0]), dtype=torch.float32)
         # Work around LightningModule property access during JIT introspection
         try:
 
             def _mark_scripting_safe(mod):
                 try:
                     if hasattr(mod, "_jit_is_scripting"):
                         setattr(mod, "_jit_is_scripting", True)
                 except Exception:
                     pass
                 try:
                     for _name, _child in getattr(mod, "named_modules", lambda: [])():
                         try:
                             if hasattr(_child, "_jit_is_scripting"):
@@ -454,215 +540,250 @@ def train_deeptica(
         Optional per-pair weights (e.g., scaled-time or bias reweighting).
     """
 
     import time as _time
 
     t0 = _time.time()
     # Deterministic behavior
     set_all_seeds(int(getattr(cfg, "seed", 2024)))
     # Prepare features and fit external scaler (float32 pipeline)
     X = np.concatenate([np.asarray(x, dtype=np.float32) for x in X_list], axis=0)
     scaler = StandardScaler(with_mean=True, with_std=True).fit(
         np.asarray(X, dtype=np.float64)
     )
     # Transform, then switch to float32 for training
     Z = scaler.transform(np.asarray(X, dtype=np.float64)).astype(np.float32, copy=False)
 
     # Build network with official constructor; disable internal normalization
     in_dim = int(Z.shape[1])
     hidden_cfg = tuple(int(h) for h in getattr(cfg, "hidden", ()) or ())
     if bool(getattr(cfg, "linear_head", False)):
         hidden_layers: tuple[int, ...] = ()
     else:
         hidden_layers = hidden_cfg if hidden_cfg else (32, 16)
     layers = [in_dim, *hidden_layers, int(cfg.n_out)]
     activation_name = str(getattr(cfg, "activation", "gelu")).lower().strip() or "gelu"
+    hidden_dropout_cfg: Any = getattr(cfg, "hidden_dropout", ())
+    layer_norm_hidden = bool(getattr(cfg, "layer_norm_hidden", False))
     try:
         core = DeepTICA(
             layers=layers,
             n_cvs=int(cfg.n_out),
             activation=activation_name,
             options={"norm_in": False},
         )
     except TypeError:
         core = DeepTICA(
             layers=layers,
             n_cvs=int(cfg.n_out),
             options={"norm_in": False},
         )
         _override_core_mlp(
-            core, layers, activation_name, bool(getattr(cfg, "linear_head", False))
+            core,
+            layers,
+            activation_name,
+            bool(getattr(cfg, "linear_head", False)),
+            hidden_dropout=hidden_dropout_cfg,
+            layer_norm_hidden=layer_norm_hidden,
         )
     else:
         _override_core_mlp(
-            core, layers, activation_name, bool(getattr(cfg, "linear_head", False))
+            core,
+            layers,
+            activation_name,
+            bool(getattr(cfg, "linear_head", False)),
+            hidden_dropout=hidden_dropout_cfg,
+            layer_norm_hidden=layer_norm_hidden,
         )
     # Wrap with input LayerNorm and light dropout for stability on tiny nets
     import torch.nn as _nn  # type: ignore
 
     def _strip_batch_norm(module: _nn.Module) -> None:
         for name, child in module.named_children():
             if isinstance(child, _nn.modules.batchnorm._BatchNorm):
                 setattr(module, name, _nn.Identity())
             else:
                 _strip_batch_norm(child)
 
     class _PrePostWrapper(_nn.Module):  # type: ignore[misc]
         def __init__(self, inner, in_features: int, *, ln_in: bool, p_drop: float):
             super().__init__()
             self.ln = _nn.LayerNorm(in_features) if ln_in else _nn.Identity()
-            p = float(max(0.0, min(0.1, p_drop)))
+            p = float(max(0.0, min(1.0, p_drop)))
             self.drop_in = _nn.Dropout(p=p) if p > 0 else _nn.Identity()
             self.inner = inner
             self.drop_out = _nn.Identity()
 
         def forward(self, x):  # type: ignore[override]
             x = self.ln(x)
             x = self.drop_in(x)
             return self.inner(x)
 
-    class _WhitenWrapper(_nn.Module):  # type: ignore[misc]
-        def __init__(
-            self,
-            inner,
-            mean: np.ndarray | torch.Tensor,
-            transform: np.ndarray | torch.Tensor,
-        ):
-            super().__init__()
-            self.inner = inner
-            self.register_buffer("mean", torch.as_tensor(mean, dtype=torch.float32))
-            self.register_buffer(
-                "transform", torch.as_tensor(transform, dtype=torch.float32)
-            )
-
-        def forward(self, x):  # type: ignore[override]
-            y = self.inner(x)
-            y = y - self.mean
-            return torch.matmul(y, self.transform.T)
-
     _strip_batch_norm(core)
+    dropout_in = getattr(cfg, "dropout_input", None)
+    if dropout_in is None:
+        dropout_in = getattr(cfg, "dropout", 0.0)
+    dropout_in = float(max(0.0, min(1.0, float(dropout_in))))
     net = _PrePostWrapper(
         core,
         in_dim,
-        ln_in=bool(getattr(cfg, "layer_norm_in", True)),
-        p_drop=float(getattr(cfg, "dropout", 0.1)),
+        ln_in=bool(getattr(cfg, "layer_norm_in", False)),
+        p_drop=dropout_in,
     )
     torch.manual_seed(int(cfg.seed))
 
+    tau_schedule = tuple(
+        int(x)
+        for x in (getattr(cfg, "tau_schedule", ()) or ())
+        if int(x) > 0
+    )
+    if not tau_schedule:
+        tau_schedule = (int(cfg.lag),)
+
     idx_t, idx_tlag = pairs
 
     # Validate or construct per-shard pairs to ensure x_t != x_{t+tau}
     def _build_uniform_pairs_per_shard(
         blocks: List[np.ndarray], lag: int
     ) -> tuple[np.ndarray, np.ndarray]:
         L = max(1, int(lag))
         i_parts: List[np.ndarray] = []
         j_parts: List[np.ndarray] = []
         off = 0
         for b in blocks:
             n = int(np.asarray(b).shape[0])
             if n > L:
                 i = np.arange(0, n - L, dtype=np.int64)
                 j = i + L
                 i_parts.append(off + i)
                 j_parts.append(off + j)
             off += n
         if not i_parts:
             return np.asarray([], dtype=np.int64), np.asarray([], dtype=np.int64)
         return (
             np.concatenate(i_parts).astype(np.int64, copy=False),
             np.concatenate(j_parts).astype(np.int64, copy=False),
         )
 
     def _needs_repair(i: np.ndarray | None, j: np.ndarray | None) -> bool:
         if i is None or j is None:
             return True
         if i.size == 0 or j.size == 0:
             return True
         try:
             d = np.asarray(j, dtype=np.int64) - np.asarray(i, dtype=np.int64)
             if d.size == 0:
                 return True
             return bool(np.min(d) <= 0)
         except Exception:
             return True
 
-    if _needs_repair(idx_t, idx_tlag):
-        idx_t, idx_tlag = _build_uniform_pairs_per_shard(X_list, int(cfg.lag))
+    if len(tau_schedule) > 1:
+        idx_parts: List[np.ndarray] = []
+        j_parts: List[np.ndarray] = []
+        for tau_val in tau_schedule:
+            i_tau, j_tau = _build_uniform_pairs_per_shard(X_list, int(tau_val))
+            if i_tau.size and j_tau.size:
+                idx_parts.append(i_tau)
+                j_parts.append(j_tau)
+        if idx_parts:
+            idx_t = np.concatenate(idx_parts).astype(np.int64, copy=False)
+            idx_tlag = np.concatenate(j_parts).astype(np.int64, copy=False)
+        else:
+            idx_t = np.asarray([], dtype=np.int64)
+            idx_tlag = np.asarray([], dtype=np.int64)
+    else:
+        if _needs_repair(idx_t, idx_tlag):
+            idx_t, idx_tlag = _build_uniform_pairs_per_shard(
+                X_list, int(tau_schedule[0])
+            )
+
+    idx_t = np.asarray(idx_t, dtype=np.int64)
+    idx_tlag = np.asarray(idx_tlag, dtype=np.int64)
 
     # Simple telemetry: evaluate a proxy objective before and after training.
     def _vamp2_proxy(Y: np.ndarray, i: np.ndarray, j: np.ndarray) -> float:
         if Y.size == 0 or i.size == 0:
             return 0.0
         A = Y[i]
         B = Y[j]
         # Mean-center
         A = A - np.mean(A, axis=0, keepdims=True)
         B = B - np.mean(B, axis=0, keepdims=True)
         # Normalize columns
         A_std = np.std(A, axis=0, ddof=1) + 1e-12
         B_std = np.std(B, axis=0, ddof=1) + 1e-12
         A = A / A_std
         B = B / B_std
         # Component-wise Pearson r, squared, averaged across outputs
         num = np.sum(A * B, axis=0)
         den = A.shape[0] - 1
         r = num / max(1.0, den)
         return float(np.mean(r * r))
 
     # Objective before training using current net init
     with torch.no_grad():
         try:
             Y0 = net(Z)  # type: ignore[misc]
         except Exception:
             # Best-effort: convert to torch tensor if required by the backend
             Y0 = net(torch.as_tensor(Z, dtype=torch.float32)).detach().cpu().numpy()  # type: ignore[assignment]
         if isinstance(Y0, torch.Tensor):
             Y0 = Y0.detach().cpu().numpy()
     obj_before = _vamp2_proxy(
         Y0, np.asarray(idx_t, dtype=int), np.asarray(idx_tlag, dtype=int)
     )
 
     # Build time-lagged dataset for training
     ds = None
     try:
         # Normalize index arrays and construct default weights (ones) when not provided
         if idx_t is None or idx_tlag is None or (len(idx_t) == 0 or len(idx_tlag) == 0):
             n = int(Z.shape[0])
-            L = int(cfg.lag)
+            L = int(tau_schedule[-1])
             if L < n:
                 idx_t = np.arange(0, n - L, dtype=int)
                 idx_tlag = idx_t + L
             else:
                 idx_t = np.asarray([], dtype=int)
                 idx_tlag = np.asarray([], dtype=int)
         idx_t = np.asarray(idx_t, dtype=int)
         idx_tlag = np.asarray(idx_tlag, dtype=int)
         if weights is None:
             weights_arr = np.ones((int(idx_t.shape[0]),), dtype=np.float32)
         else:
             weights_arr = np.asarray(weights, dtype=np.float32).reshape(-1)
+            if weights_arr.size == 1 and int(idx_t.shape[0]) > 1:
+                weights_arr = np.full(
+                    (int(idx_t.shape[0]),),
+                    float(weights_arr[0]),
+                    dtype=np.float32,
+                )
+            elif int(idx_t.shape[0]) != int(weights_arr.shape[0]):
+                raise ValueError(
+                    "weights must have the same length as the number of lagged pairs"
+                )
 
         # Ensure explicit float32 tensors for lagged pairs
         # If you use a scaler, after scaler.fit, cast outputs to float32
         # using torch tensors to standardize dtype end-to-end.
         try:
             x_t_np = Z[idx_t]
             x_tau_np = Z[idx_tlag]
             x_t_tensor = torch.as_tensor(x_t_np, dtype=torch.float32)
             x_tau_tensor = torch.as_tensor(x_tau_np, dtype=torch.float32)
         except Exception:
             # Fallback via precomputed Z
             x_t_tensor = torch.as_tensor(Z[idx_t], dtype=torch.float32)
             x_tau_tensor = torch.as_tensor(Z[idx_tlag], dtype=torch.float32)
 
         # Preflight assertions: pairs must differ and weights must be positive on average
         try:
             n_pairs = int(x_t_tensor.shape[0])
             if n_pairs > 0:
                 sel = np.random.default_rng(int(cfg.seed)).choice(
                     n_pairs, size=min(256, n_pairs), replace=False
                 )
                 xa = x_t_tensor[sel]
                 xb = x_tau_tensor[sel]
                 if torch.allclose(xa, xb):
                     raise ValueError(
@@ -1007,65 +1128,74 @@ def train_deeptica(
                             "log_dir",
                             Path(checkpoints_root) / "deeptica" / version_str,
                         )
                     ).resolve()
                     metrics_csv_path = log_dir / "metrics.csv"
                 except Exception:
                     metrics_csv_path = None
             if TensorBoardLoggerCls is not None:
                 tb_logger = TensorBoardLoggerCls(
                     save_dir=str(Path.cwd() / "runs"), name="deeptica_tb"
                 )
                 loggers.append(tb_logger)
         except Exception:
             pass
 
         # Enable progress bar via env flag when desired
         _pb_env = str(_os.getenv("PMARLO_MLCV_PROGRESS", "0")).strip().lower()
         _pb = _pb_env in {"1", "true", "yes", "on"}
         # Wrap underlying model in a LightningModule so PL Trainer can optimize it
         try:
             try:
                 import pytorch_lightning as pl  # type: ignore
             except Exception:
                 import lightning.pytorch as pl  # type: ignore
 
+            vamp_kwargs = {
+                "eps": float(max(1e-9, getattr(cfg, "vamp_eps", 1e-3))),
+                "eps_abs": float(max(0.0, getattr(cfg, "vamp_eps_abs", 1e-6))),
+                "alpha": float(
+                    min(max(getattr(cfg, "vamp_alpha", 0.15), 0.0), 1.0)
+                ),
+                "cond_reg": float(max(0.0, getattr(cfg, "vamp_cond_reg", 1e-4))),
+            }
+
             class DeepTICALightningWrapper(pl.LightningModule):  # type: ignore
                 def __init__(
                     self,
                     inner,
                     lr: float,
                     weight_decay: float,
                     history_dir: str | None = None,
                     *,
                     lr_schedule: str = "cosine",
                     warmup_epochs: int = 5,
                     max_epochs: int = 200,
                 ):
                     super().__init__()
                     self.inner = inner
-                    self.vamp_loss = VAMP2Loss(eps=1e-5)
+                    self.vamp_loss = VAMP2Loss(**vamp_kwargs)
                     self._train_loss_accum: list[float] = []
                     self._val_loss_accum: list[float] = []
                     self._val_score_accum: list[float] = []
                     self._grad_norm_accum: list[float] = []
                     self._val_var_z0_accum: list[list[float]] = []
                     self._val_var_zt_accum: list[list[float]] = []
                     self._cond_c0_accum: list[float] = []
                     self._cond_ctt_accum: list[float] = []
                     self.train_loss_curve: list[float] = []
                     self.val_loss_curve: list[float] = []
                     self.val_score_curve: list[float] = []
                     self.var_z0_curve: list[list[float]] = []
                     self.var_zt_curve: list[list[float]] = []
                     self.cond_c0_curve: list[float] = []
                     self.cond_ctt_curve: list[float] = []
                     self.grad_norm_curve: list[float] = []
                     self._train_loss_accum: list[float] = []
                     self._val_loss_accum: list[float] = []
                     self._val_score_accum: list[float] = []
                     self.train_loss_curve: list[float] = []
                     self.val_loss_curve: list[float] = []
                     self.val_score_curve: list[float] = []
                     # keep hparams for checkpointing/logging
                     self.save_hyperparameters(
                         {
@@ -1438,55 +1568,55 @@ def train_deeptica(
                             torch.tensor(
                                 avg_cond_c0, device=self.device, dtype=torch.float32
                             ),
                             prog_bar=False,
                         )
                     if self._cond_ctt_accum:
                         avg_cond_ctt = float(
                             sum(self._cond_ctt_accum) / len(self._cond_ctt_accum)
                         )
                         self.cond_ctt_curve.append(avg_cond_ctt)
                         self.log(
                             "cond_Ctt",
                             torch.tensor(
                                 avg_cond_ctt, device=self.device, dtype=torch.float32
                             ),
                             prog_bar=False,
                         )
                     self._val_loss_accum.clear()
                     self._val_score_accum.clear()
                     self._val_var_z0_accum.clear()
                     self._val_var_zt_accum.clear()
                     self._cond_c0_accum.clear()
                     self._cond_ctt_accum.clear()
 
                 def configure_optimizers(self):  # type: ignore[override]
-                    # Adam with mild weight decay for stability
+                    # AdamW with mild weight decay for stability
                     weight_decay = float(self.hparams.weight_decay)
                     if weight_decay <= 0.0:
                         weight_decay = 1e-4
-                    opt = torch.optim.Adam(
+                    opt = torch.optim.AdamW(
                         self.parameters(),
                         lr=float(self.hparams.lr),
                         weight_decay=weight_decay,
                     )
                     sched_name = (
                         str(getattr(self.hparams, "lr_schedule", "cosine"))
                         if hasattr(self, "hparams")
                         else "cosine"
                     )
                     warmup = (
                         int(getattr(self.hparams, "warmup_epochs", 5))
                         if hasattr(self, "hparams")
                         else 5
                     )
                     maxe = (
                         int(getattr(self.hparams, "max_epochs", 200))
                         if hasattr(self, "hparams")
                         else 200
                     )
                     if sched_name == "cosine":
                         try:
                             import math as _math  # noqa: F401
 
                             from torch.optim.lr_scheduler import (  # type: ignore
                                 CosineAnnealingLR,
@@ -1534,106 +1664,114 @@ def train_deeptica(
             # Choose a persistent directory for per-epoch JSONL logging
             try:
                 hist_dir = (
                     ckpt_dir
                     if "ckpt_dir" in locals() and ckpt_dir is not None
                     else (Path.cwd() / "runs" / "deeptica" / str(int(t0)))
                 )
             except Exception:
                 hist_dir = None
             wrapped = DeepTICALightningWrapper(
                 net,
                 lr=float(cfg.learning_rate),
                 weight_decay=float(cfg.weight_decay),
                 history_dir=str(hist_dir) if hist_dir is not None else None,
                 lr_schedule=str(getattr(cfg, "lr_schedule", "cosine")),
                 warmup_epochs=int(getattr(cfg, "warmup_epochs", 5)),
                 max_epochs=int(getattr(cfg, "max_epochs", 200)),
             )
         except Exception:
             # If Lightning is completely unavailable, fall back to model.fit (handled below)
             wrapped = net
 
         # Enforce minimum training duration to avoid early flat-zero stalls
         _max_epochs = int(getattr(cfg, "max_epochs", 200))
         _min_epochs = max(1, min(50, _max_epochs // 4))
-        trainer = Trainer(
-            max_epochs=_max_epochs,
-            min_epochs=_min_epochs,
-            enable_progress_bar=_pb,
-            logger=loggers if loggers else False,
-            callbacks=callbacks,
-            deterministic=True,
-            log_every_n_steps=1,
-            enable_checkpointing=True,
-            gradient_clip_val=2.0,
-        )
+        clip_val = float(max(0.0, getattr(cfg, "gradient_clip_val", 0.0)))
+        clip_alg = str(getattr(cfg, "gradient_clip_algorithm", "norm"))
+        trainer_kwargs = {
+            "max_epochs": _max_epochs,
+            "min_epochs": _min_epochs,
+            "enable_progress_bar": _pb,
+            "logger": loggers if loggers else False,
+            "callbacks": callbacks,
+            "deterministic": True,
+            "log_every_n_steps": 1,
+            "enable_checkpointing": True,
+            "gradient_clip_val": clip_val,
+            "gradient_clip_algorithm": clip_alg,
+        }
+        try:
+            trainer = Trainer(**trainer_kwargs)
+        except TypeError:
+            trainer_kwargs.pop("gradient_clip_algorithm", None)
+            trainer = Trainer(**trainer_kwargs)
 
         if dm is not None:
             trainer.fit(model=wrapped, datamodule=dm)
         else:
             trainer.fit(
                 model=wrapped,
                 train_dataloaders=train_loader,
                 val_dataloaders=val_loader,
             )
 
         # Persist artifacts info
         try:
             if ckpt_callback is not None and getattr(
                 ckpt_callback, "best_model_path", None
             ):
                 best_path = str(getattr(ckpt_callback, "best_model_path"))
             else:
                 best_path = None
             if ckpt_callback_corr is not None and getattr(
                 ckpt_callback_corr, "best_model_path", None
             ):
                 best_path_corr = str(getattr(ckpt_callback_corr, "best_model_path"))
             else:
                 best_path_corr = None
         except Exception:
             best_path = None
             best_path_corr = None
     else:
         # Fallback: if the model exposes a .fit(...) method, use it (older mlcolvar)
         if hasattr(net, "fit"):
             try:
                 getattr(net, "fit")(
                     ds,
                     batch_size=int(cfg.batch_size),
                     max_epochs=int(cfg.max_epochs),
                     early_stopping_patience=int(cfg.early_stopping),
                     shuffle=False,
                 )
             except TypeError:
                 # Older API: pass arrays and indices directly
                 # Ensure weights are always provided (mlcolvar>=1.2 may require them)
                 _w = weights_arr
                 getattr(net, "fit")(
                     Z,
-                    lagtime=int(cfg.lag),
+                    lagtime=int(tau_schedule[-1]),
                     idx_t=np.asarray(idx_t, dtype=int),
                     idx_tlag=np.asarray(idx_tlag, dtype=int),
                     weights=_w,
                     batch_size=int(cfg.batch_size),
                     max_epochs=int(cfg.max_epochs),
                     early_stopping_patience=int(cfg.early_stopping),
                     shuffle=False,
                 )
             except Exception:
                 # Last-resort minimal loop: no-op to avoid crash; metrics will reflect proxy objective only
                 pass
         else:
             raise ImportError(
                 "Lightning (lightning or pytorch_lightning) is required for Deep-TICA training"
             )
     net, whitening_info = _apply_output_whitening(net, Z, idx_tlag, apply=False)
     net.eval()
     with torch.no_grad():
         try:
             Y1 = net(Z)  # type: ignore[misc]
         except Exception:
             Y1 = net(torch.as_tensor(Z, dtype=torch.float32)).detach().cpu().numpy()  # type: ignore[assignment]
         if isinstance(Y1, torch.Tensor):
             Y1 = Y1.detach().cpu().numpy()
     obj_after = _vamp2_proxy(
@@ -1723,50 +1861,51 @@ def train_deeptica(
         var_z0_curve = []
     if var_zt_curve is None:
         var_zt_curve = []
     if cond_c0_curve is None:
         cond_c0_curve = []
     if cond_ctt_curve is None:
         cond_ctt_curve = []
     if grad_norm_curve is None:
         grad_norm_curve = []
 
     history = {
         "loss_curve": train_curve,
         "val_loss_curve": val_curve,
         "objective_curve": score_curve,
         "val_score_curve": score_curve,
         "val_score": score_curve,
         "var_z0_curve": var_z0_curve,
         "var_zt_curve": var_zt_curve,
         "cond_c00_curve": cond_c0_curve,
         "cond_ctt_curve": cond_ctt_curve,
         "grad_norm_curve": grad_norm_curve,
         "initial_objective": float(obj_before),
         "epochs": history_epochs,
         "log_every": int(cfg.log_every),
         "wall_time_s": float(max(0.0, _time.time() - t0)),
+        "tau_schedule": [int(x) for x in tau_schedule],
     }
 
     history["output_variance"] = whitening_info.get("output_variance")
     history["output_mean"] = whitening_info.get("mean")
     history["output_transform"] = whitening_info.get("transform")
     history["output_transform_applied"] = whitening_info.get("transform_applied")
 
     if history.get("var_z0_curve"):
         history["var_z0_curve"][-1] = whitening_info.get("output_variance")
     else:
         history["var_z0_curve"] = [whitening_info.get("output_variance")]
 
     if history.get("var_zt_curve"):
         history["var_zt_curve"][-1] = whitening_info.get("var_zt")
     else:
         history["var_zt_curve"] = [whitening_info.get("var_zt")]
 
     if history.get("cond_c00_curve"):
         history["cond_c00_curve"][-1] = whitening_info.get("cond_c00")
     else:
         history["cond_c00_curve"] = [whitening_info.get("cond_c00")]
 
     if history.get("cond_ctt_curve"):
         history["cond_ctt_curve"][-1] = whitening_info.get("cond_ctt")
     else:
diff --git a/src/pmarlo/features/deeptica/losses.py b/src/pmarlo/features/deeptica/losses.py
index 860ee2ed7981c899b047cbaf04c15b1b11f46460..a09db5cc620d6cb232d4a8795ac1ab59ad5174d8 100644
--- a/src/pmarlo/features/deeptica/losses.py
+++ b/src/pmarlo/features/deeptica/losses.py
@@ -1,95 +1,142 @@
 from __future__ import annotations
 
 """Numerically stable VAMP-2 loss utilities for Deep-TICA training."""
 
 from typing import Optional, Tuple
 
 import torch
 from torch import Tensor, nn
 
 
 class VAMP2Loss(nn.Module):
     """Compute a scale-invariant, regularised VAMP-2 score."""
 
-    def __init__(self, eps: float = 1e-6, dtype: torch.dtype = torch.float64) -> None:
+    def __init__(
+        self,
+        eps: float = 1e-3,
+        *,
+        eps_abs: float = 1e-6,
+        alpha: float = 0.15,
+        cond_reg: float = 1e-4,
+        jitter: float = 1e-8,
+        max_cholesky_retries: int = 5,
+        jitter_growth: float = 10.0,
+        dtype: torch.dtype = torch.float64,
+    ) -> None:
         super().__init__()
         self.eps = float(eps)
+        self.eps_abs = float(max(eps_abs, 0.0))
+        self.alpha = float(min(max(alpha, 0.0), 1.0))
+        self.cond_reg = float(max(cond_reg, 0.0))
+        self.jitter = float(max(jitter, 0.0))
+        self.jitter_growth = float(max(jitter_growth, 1.0))
+        self.max_cholesky_retries = int(max(1, max_cholesky_retries))
         self.target_dtype = dtype
         self.register_buffer("_eye", torch.empty(0, dtype=dtype), persistent=False)
 
     def forward(
         self,
         z0: Tensor,
         zt: Tensor,
         weights: Optional[Tensor] = None,
     ) -> Tuple[Tensor, Tensor]:
         if z0.ndim != 2 or zt.ndim != 2:
             raise ValueError("VAMP2Loss expects 2-D activations")
         if z0.shape != zt.shape:
             raise ValueError("z0 and zt must share the same shape")
         if z0.shape[0] == 0:
             raise ValueError("VAMP2Loss received empty batch")
 
         device = z0.device
         dtype = self.target_dtype
         z0 = z0.to(dtype=dtype)
         zt = zt.to(dtype=dtype)
 
         if weights is None:
             w = torch.full(
                 (z0.shape[0], 1), 1.0 / float(z0.shape[0]), device=device, dtype=dtype
             )
         else:
             w = weights.reshape(-1, 1).to(device=device, dtype=dtype)
             w = torch.clamp(w, min=0.0)
             total = torch.clamp(w.sum(), min=1e-12)
             w = w / total
 
         mean0 = torch.sum(z0 * w, dim=0, keepdim=True)
         meant = torch.sum(zt * w, dim=0, keepdim=True)
         z0_c = z0 - mean0
         zt_c = zt - meant
 
         C00 = z0_c.T @ (z0_c * w)
         Ctt = zt_c.T @ (zt_c * w)
         C0t = z0_c.T @ (zt_c * w)
 
         eye = self._identity_like(C00, device)
         dim = C00.shape[-1]
         trace_floor = torch.tensor(1e-12, device=device, dtype=dtype)
         tr0 = torch.clamp(torch.trace(C00), min=trace_floor)
         trt = torch.clamp(torch.trace(Ctt), min=trace_floor)
         mu0 = tr0 / float(max(1, dim))
         mut = trt / float(max(1, dim))
-        ridge0 = mu0 * self.eps
-        ridget = mut * self.eps
-        alpha = 0.02
+        diag_mean0 = torch.clamp(torch.diagonal(C00, dim1=-2, dim2=-1).mean(), min=trace_floor)
+        diag_meant = torch.clamp(torch.diagonal(Ctt, dim1=-2, dim2=-1).mean(), min=trace_floor)
+        ridge0 = torch.maximum(mu0 * self.eps, diag_mean0 * self.eps_abs)
+        ridget = torch.maximum(mut * self.eps, diag_meant * self.eps_abs)
+        alpha = self.alpha
         C00 = (1.0 - alpha) * C00 + (alpha * mu0 + ridge0) * eye
         Ctt = (1.0 - alpha) * Ctt + (alpha * mut + ridget) * eye
         C00 = (C00 + C00.T) * 0.5
         Ctt = (Ctt + Ctt.T) * 0.5
 
-        L0 = torch.linalg.cholesky(C00, upper=False)
-        Lt = torch.linalg.cholesky(Ctt, upper=False)
+        L0, C00 = self._stable_cholesky(C00, eye)
+        Lt, Ctt = self._stable_cholesky(Ctt, eye)
 
         try:
             left = torch.linalg.solve_triangular(L0, C0t, upper=False)
             right = torch.linalg.solve_triangular(
                 Lt, left.transpose(-1, -2), upper=False
             )
         except AttributeError:  # pragma: no cover - legacy torch fallback
             left = torch.triangular_solve(C0t, L0, upper=False)[0]
             right = torch.triangular_solve(left.transpose(-1, -2), Lt, upper=False)[0]
         K = right.transpose(-1, -2)
 
         score = torch.sum(K * K)
-        loss = -score
+
+        penalty = torch.tensor(0.0, device=device, dtype=dtype)
+        if self.cond_reg > 0.0:
+            eig0 = torch.linalg.eigvalsh(C00)
+            eigt = torch.linalg.eigvalsh(Ctt)
+            cond_c00 = eig0.max() / torch.clamp(eig0.min(), min=trace_floor)
+            cond_ctt = eigt.max() / torch.clamp(eigt.min(), min=trace_floor)
+            cond_term = torch.log(torch.clamp(cond_c00, min=1.0)) + torch.log(
+                torch.clamp(cond_ctt, min=1.0)
+            )
+            penalty = penalty + self.cond_reg * cond_term
+
+        loss = -score + penalty
         return loss, score.detach()
 
     def _identity_like(self, mat: Tensor, device: torch.device) -> Tensor:
         dim = mat.shape[-1]
         eye = self._eye
         if eye.numel() != dim * dim or eye.device != device:
             eye = torch.eye(dim, device=device, dtype=self.target_dtype)
             self._eye = eye
         return eye
+
+    def _stable_cholesky(self, mat: Tensor, eye: Tensor) -> Tuple[Tensor, Tensor]:
+        """Compute a Cholesky factor with adaptive jitter for stability."""
+
+        updated = mat
+        jitter = self.jitter
+        for attempt in range(self.max_cholesky_retries):
+            try:
+                return torch.linalg.cholesky(updated, upper=False), updated
+            except RuntimeError:
+                if attempt + 1 >= self.max_cholesky_retries:
+                    raise
+                jitter = jitter if jitter > 0.0 else 1e-12
+                updated = updated + eye * jitter
+                jitter *= self.jitter_growth
+        raise RuntimeError("Cholesky decomposition failed after retries")
diff --git a/src/pmarlo/features/deeptica_trainer.py b/src/pmarlo/features/deeptica_trainer.py
index fb9d0cddafa10d737fb431de38e57d05a90114e1..13549f78b112ff776bed6a01be084c53c638a7bb 100644
--- a/src/pmarlo/features/deeptica_trainer.py
+++ b/src/pmarlo/features/deeptica_trainer.py
@@ -1,75 +1,87 @@
 from __future__ import annotations
 
 """DeepTICA trainer integrating VAMP-2 optimisation and curriculum support."""
 
 import logging
 from dataclasses import dataclass
 from pathlib import Path
 from typing import Dict, List, Optional, Sequence, Tuple
 
 import numpy as np
 import torch
 from torch.nn.utils import clip_grad_norm_
 
 from .deeptica.losses import VAMP2Loss
 
 __all__ = ["TrainerConfig", "DeepTICATrainer"]
 
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass(frozen=True)
 class TrainerConfig:
     tau_steps: int
-    learning_rate: float = 1e-3
+    learning_rate: float = 3e-4
     weight_decay: float = 0.0
     use_weights: bool = True
     tau_schedule: Tuple[int, ...] = ()
-    grad_clip_norm: Optional[float] = 2.0
+    grad_clip_norm: Optional[float] = 1.0
     log_every: int = 25
     checkpoint_dir: Optional[Path] = None
     checkpoint_metric: str = "vamp2"
     device: str = "auto"
     scheduler: str = "none"  # "none" | "cosine"
+    scheduler_warmup_steps: int = 0
+    scheduler_total_steps: Optional[int] = None
+    max_steps: Optional[int] = None
+    vamp_eps: float = 1e-3
+    vamp_eps_abs: float = 1e-6
+    vamp_alpha: float = 0.15
+    vamp_cond_reg: float = 1e-4
 
 
 class DeepTICATrainer:
     """Optimises a DeepTICA model using a tau-curriculum and VAMP-2 loss."""
 
     def __init__(self, model, cfg: TrainerConfig) -> None:
         if cfg.tau_steps <= 0:
             raise ValueError("tau_steps must be positive")
         self.model = model
         self.cfg = cfg
 
         device_str = self._resolve_device(cfg.device)
         self.device = torch.device(device_str)
         self.model.net.to(self.device)
         self.model.net.train()
-        self.loss_module = VAMP2Loss(eps=1e-5).to(self.device)
+        self.loss_module = VAMP2Loss(
+            eps=float(max(1e-9, cfg.vamp_eps)),
+            eps_abs=float(max(0.0, cfg.vamp_eps_abs)),
+            alpha=float(min(max(cfg.vamp_alpha, 0.0), 1.0)),
+            cond_reg=float(max(0.0, cfg.vamp_cond_reg)),
+        ).to(self.device)
 
         weight_decay = float(cfg.weight_decay)
         if weight_decay <= 0.0:
             weight_decay = 1e-4
         self.optimizer = torch.optim.AdamW(
             self.model.net.parameters(),
             lr=float(cfg.learning_rate),
             weight_decay=weight_decay,
         )
 
         self.scheduler = self._make_scheduler()
 
         self.curriculum: List[int] = (
             list(cfg.tau_schedule) if cfg.tau_schedule else [int(cfg.tau_steps)]
         )
         self.curriculum_index = 0
 
         self.history: List[Dict[str, float]] = []
         self.global_step: int = 0
         self.best_score: float = float("-inf")
         self.checkpoint_dir = Path(cfg.checkpoint_dir) if cfg.checkpoint_dir else None
         if self.checkpoint_dir:
             self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
             self.best_checkpoint_path = self.checkpoint_dir / "best_deeptica.pt"
         else:
@@ -137,52 +149,85 @@ class DeepTICATrainer:
     ) -> Dict[str, float]:
         tensors = self._prepare_batch(batch)
         if tensors is None:
             return {"loss": 0.0, "vamp2": 0.0, "tau": float(self.current_tau())}
         x_t, x_tau, weights = tensors
         self.model.net.eval()
         with torch.no_grad():
             loss, score = self._compute_loss_and_score(x_t, x_tau, weights)
         return {
             "loss": float(loss.item()),
             "vamp2": float(score.item()),
             "tau": float(self.current_tau()),
         }
 
     # ------------------------------------------------------------------
     # Internal helpers
     # ------------------------------------------------------------------
     def _resolve_device(self, device_spec: str) -> str:
         if device_spec.lower() == "auto":
             return "cuda" if torch.cuda.is_available() else "cpu"
         return device_spec
 
     def _make_scheduler(self):
         if self.cfg.scheduler.lower() != "cosine":
             return None
-        t_max = max(1, int(1000))
-        return torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=t_max)
+        try:
+            from torch.optim.lr_scheduler import (  # type: ignore[attr-defined]
+                CosineAnnealingLR,
+                LambdaLR,
+                SequentialLR,
+            )
+        except Exception:
+            return None
+
+        warmup_steps = max(0, int(getattr(self.cfg, "scheduler_warmup_steps", 0)))
+        total_steps = getattr(self.cfg, "scheduler_total_steps", None)
+        if total_steps is None or int(total_steps) <= 0:
+            total_steps = getattr(self.cfg, "max_steps", None)
+        if total_steps is None or int(total_steps) <= 0:
+            total_steps = getattr(self.cfg, "log_every", 0) * 10
+        if total_steps is None or int(total_steps) <= 0:
+            total_steps = self.cfg.tau_steps
+        total_steps = max(1, int(total_steps))
+
+        schedulers = []
+        milestones: List[int] = []
+        if warmup_steps > 0:
+
+            def _lr_lambda(step: int) -> float:
+                return min(1.0, float(step + 1) / float(max(1, warmup_steps)))
+
+            schedulers.append(LambdaLR(self.optimizer, lr_lambda=_lr_lambda))
+            milestones.append(int(warmup_steps))
+
+        t_max = max(1, total_steps - warmup_steps)
+        schedulers.append(CosineAnnealingLR(self.optimizer, T_max=t_max))
+
+        if len(schedulers) == 1:
+            return schedulers[0]
+        return SequentialLR(self.optimizer, schedulers, milestones=milestones)
 
     def _prepare_batch(
         self,
         batch: List[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]],
     ) -> Optional[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:
         x_parts: List[torch.Tensor] = []
         y_parts: List[torch.Tensor] = []
         w_parts: List[torch.Tensor] = []
 
         for x_t, x_tau, weights in batch:
             x_arr = torch.as_tensor(x_t, dtype=torch.float32, device=self.device)
             y_arr = torch.as_tensor(x_tau, dtype=torch.float32, device=self.device)
             if x_arr.ndim != 2 or y_arr.ndim != 2:
                 raise ValueError("Batch entries must be 2-D arrays")
             if x_arr.shape != y_arr.shape:
                 raise ValueError("x_t and x_tau must have matching shapes")
             x_parts.append(x_arr)
             y_parts.append(y_arr)
 
             if self.cfg.use_weights:
                 if weights is None:
                     w = torch.ones(
                         x_arr.shape[0], dtype=torch.float32, device=self.device
                     )
                 else:
