diff --git a/example_programs/app_usecase/app/app.py b/example_programs/app_usecase/app/app.py
index d47a125d41fd9d859afa03c2f848fd5fb466e598..679b41ff4ce6434149c4882252cd8c9f66905ee5 100644
--- a/example_programs/app_usecase/app/app.py
+++ b/example_programs/app_usecase/app/app.py
@@ -63,78 +63,136 @@ def _parse_temperature_ladder(raw: str) -> List[float]:
     temps: List[float] = []
     for token in cleaned.split(","):
         token = token.strip()
         if not token:
             continue
         temps.append(float(token))
     if not temps:
         raise ValueError("Provide at least one temperature in Kelvin.")
     return temps
 
 
 def _select_shard_paths(groups: Sequence[Dict[str, object]], run_ids: Sequence[str]) -> List[Path]:
     lookup: Dict[str, Sequence[str]] = {
         str(entry.get("run_id")): entry.get("paths", [])  # type: ignore[dict-item]
         for entry in groups
     }
     selected: List[Path] = []
     for run_id in run_ids:
         paths = lookup.get(run_id, [])
         for p in paths:
             selected.append(Path(p))
     return selected
 
 
 def _metrics_table(flags: Dict[str, object]) -> pd.DataFrame:
-    def _to_serializable(val: object) -> object:
-        # Keep simple scalars as-is
-        if isinstance(val, (str, int, float, bool)) or val is None:
-            return val
-        # Coerce NumPy scalars
-        try:
-            import numpy as np  # type: ignore
+    """Render build flags in a tabular, Arrow-friendly representation.
 
-            if isinstance(val, np.generic):
-                return val.item()
-        except Exception:
-            pass
-        # For sequences, mappings, arrays, and other objects, stringify to avoid Arrow type issues
-        try:
-            import json
+    Streamlit converts the returned DataFrame into an Arrow table. Mixed dtypes
+    within a column (for example booleans alongside strings) lead Arrow to infer
+    an incompatible schema which subsequently raises an ``ArrowInvalid``. The
+    build flags frequently contain boolean toggles together with nested
+    structures such as diagnostic warning lists, so we normalise them into a
+    flat table that stores display strings alongside their original type.
+    """
+
+    from collections.abc import Mapping, Sequence as _SequenceABC
+
+    try:  # Local import to avoid an unconditional dependency at module import
+        import numpy as np  # type: ignore
+    except Exception:  # pragma: no cover - NumPy is always available in practice
+        np = None  # type: ignore
 
-            return json.dumps(val, default=str)
-        except Exception:
-            return str(val)
+    def _coerce_scalar(val: object) -> object:
+        if np is not None and isinstance(val, np.generic):
+            return val.item()
+        return val
 
-    rows = []
-    for key, value in flags.items():
-        if isinstance(value, dict):
+    def _is_sequence(val: object) -> bool:
+        return isinstance(val, _SequenceABC) and not isinstance(val, (str, bytes, bytearray))
+
+    def _iter_items(prefix: str, value: object):
+        value = _coerce_scalar(value)
+
+        if isinstance(value, Mapping):
             for sub_key, sub_val in value.items():
-                rows.append({"metric": f"{key}.{sub_key}", "value": _to_serializable(sub_val)})
-        else:
-            rows.append({"metric": key, "value": _to_serializable(value)})
-    return pd.DataFrame(rows) if rows else pd.DataFrame({"metric": [], "value": []})
+                next_prefix = f"{prefix}.{sub_key}" if prefix else str(sub_key)
+                yield from _iter_items(next_prefix, sub_val)
+            return
+
+        if np is not None and hasattr(value, "tolist") and not isinstance(value, (str, bytes, bytearray)):
+            try:
+                value = value.tolist()
+            except Exception:
+                pass
+
+        if _is_sequence(value):
+            seq = list(value)
+            if not seq:
+                yield (prefix, "[]")
+                return
+            for idx, item in enumerate(seq, start=1):
+                suffix = f"[{idx}]"
+                next_prefix = f"{prefix}{suffix}" if prefix else suffix
+                yield from _iter_items(next_prefix, item)
+            return
+
+        yield (prefix, value)
+
+    def _format_value(val: object) -> tuple[str, str]:
+        val = _coerce_scalar(val)
+        if val is None:
+            return "", "NoneType"
+        if isinstance(val, bool):
+            return ("True" if val else "False"), "bool"
+        if isinstance(val, (int, float)):
+            return f"{val}", type(val).__name__
+        if isinstance(val, bytes):
+            try:
+                decoded = val.decode("utf-8")
+            except Exception:
+                decoded = val.decode("utf-8", errors="replace")
+            return decoded, "bytes"
+        return str(val), type(val).__name__
+
+    rows: List[Dict[str, object]] = []
+    for key, raw_value in flags.items():
+        for metric_key, metric_value in _iter_items(key, raw_value):
+            display, dtype_name = _format_value(metric_value)
+            rows.append({
+                "metric": metric_key,
+                "value": display,
+                "value_type": dtype_name,
+            })
+
+    if not rows:
+        return pd.DataFrame({"metric": [], "value": [], "value_type": []})
+
+    df = pd.DataFrame(rows)
+    df["value"] = pd.Series(df["value"], dtype="string")
+    df["value_type"] = pd.Series(df["value_type"], dtype="string")
+    return df
 
 
 def _format_tau_schedule(values: Sequence[int]) -> str:
     if not values:
         return ""
     return ", ".join(str(int(v)) for v in values)
 
 
 def _parse_tau_schedule(raw: str) -> List[int]:
     cleaned = raw.replace(";", ",")
     values: List[int] = []
     for token in cleaned.split(","):
         token = token.strip()
         if not token:
             continue
         try:
             val = int(token)
         except ValueError as exc:  # pragma: no cover - defensive parsing
             raise ValueError(f"Invalid tau value '{token}'") from exc
         if val <= 0:
             raise ValueError("Tau values must be positive integers.")
         values.append(val)
     if not values:
         raise ValueError("Provide at least one tau value.")
     return sorted(set(values))
